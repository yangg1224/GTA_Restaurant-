---
title: "TITLE"
author: "Adrian Wong, Yingying Zhou, Xinyi Xu, Yang Wu "
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
 bookdown::pdf_document2:
 toc: no
subtitle: "SUBTITLE"
abstract: "ABSTRACT"
thanks: 'Code and data are available at: https://github.com/yangg1224/groupproject-.git'
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
#install.packages("PerformanceAnalytics")
library("PerformanceAnalytics")
library(ggplot2)
library(kableExtra)
library(ggpubr)
```

# Introduction
```{r}

```

# Data 
In this report, we use the R statistical programming language [@citeR]. To be specific, we use the “tidyverse” package to process data [@citetidyverse], "kableExtra" package to generate tables [@citekableExtra],  "ggplot2" is used to draw diagrams [@gg1], "ggthemes" is used to change the diagram theme. [@themes]ADD ANY PACKAGE HERE. The survey conducted by Petit Poll collects the following data:

* Geographical information (FSA, Region)
* Type of restaurant (Fast food, fast casual, casual dining, premium casual, family style, fine dining)
* Number of years opened so far
* Services provided (delivery, take-out)
* Number of employees and salary of employee
* Cost to run the restaurant (Fixed, variable)
* Revenue per month
* Potential COVID-19 cases

[Appendix C](#appendixC) shows screen captures of the online version of the survey.In addition, we consulted several open datasets about the health inspection from each region, including Peel[@citePeel], Halton[@citeHalton], York [@citeYork], Durham [@citeDurham], and Toronto [@citeToronto].

The data section illustrates the intervention and data gathering methodology. Also, we describe detailed information on our dataset, with several data visualizations such as tables and graphs.


## Intervention 

In this experiment, we conducted a randomized controlled trial and randomly assigned subjects to two groups: the treatment group and the control group. Within this experiment, the treatment group receives an intervention and the control group is not being treated. 

In our case, the intervention is having restaurants open for indoor dining. Our experimental subjects are restaurants in the GTA that are currently in operation. We assumed all restaurants in Ontario are closed for indoor dining at the moment. During the shut-down, all restaurants in the control group were forbidden to offer dine-in and patio services according to the COVID-19 lockdown policy set out by the Ontario government [@citenews]. However, take-out and delivery services were acceptable. Our intervention involves randomly selecting restaurants to reopen for indoor dining in the GTA. The restaurants in the treatment and control group were picked through stratified sampling. This experiment was conducted by Petit Poll and was authorized by the Ontario Department of Public Health.

We randomized the control and treatment groups through stratified sampling. In other words, we divided all restaurants in Ontario into smaller strata. Each strata was grouped based on region. After stratification, we took random samples from each region, in proportion to its share of the population within the total population [@citeSRS]. The distribution of all strata shows as following (see more information in [Appendix A](#appendixA)):

* Toronto - 29.5%
* Durham - 12.8%
* York - 21.9%
* Peel - 24.6%
* Halton - 11%

We then use a random number generator to randomly select a certain number of restaurants from a list of restaurants that was ordered alphanumerically. This list was procured by health inspection records that indicate all of the open restaurants within each region. Procedurally, we set the minimum number and the maximum number based on the number of restaurants within the given list. If the randomly generated number was 59, we chose the 59th restaurant in the list. We repeated the process until the treatment group and the control group were established with the corresponding number of restaurants. 

To ensure the separation of treatment and non-treatment groups, we relied on stratified random sampling based on regions. After stratification, the number of restaurants in the treatment and control groups were the same. This ensures that the treatment is the only source of potential differences in outcomes between the two groups and not based on convenience of access for citizens within each region [@citeSRS]. Moreover, ad-hoc analysis showed that the proportion of restaurant types were the same between control and treatment groups. This reduces the likelihood that citizens would migrate between groups for particular restaurant types.

The experiment ran for two and a half months from December 1, 2020 to February 15th, 2021.
We distributed a consent form to 11,325 restaurants within the GTA on December 1st, 2020, and received 3,454 responses to participate in our survey by December 15th, 2020. We randomly sampled an equal number of respondents into the treatment group and control group. On December 16th, the intervention was announced for the selected 1,637 restaurants in the treatment group. We reserved half a month for the restaurant to prepare for reopening. The treatment group reopened the restaurant from January 1st to January 31st, 2021. We considered one month as the minimum effective period for a validity reopening. During the intervention, all restaurants in the treatment group were allowed to offer dine-in, patio services, delivery service, and takeout services. The survey itself was conducted for a half month from February 1st, 2021 to February 15th, 2021. Finally, we collected 3,274 responses in total from both the treatment group and control group by February 15th, 2021.

## Data Gathering Method 

The population included all restaurants in the GTA, excluding the ones that were completely closed. The sampling frame was all restaurants listed in the health inspection reporting program of all five regions. Currently, GTA has approximately 25,351 restaurants, including 7500 in Toronto, 3260 in Durham, 5553 in York, 6235 in Peel, and 2803 in Halton. We use a stratified sampling method to randomly select restaurants in each region. We sent consent forms to 11,325 restaurants, and received 3,454 responses, which reflects a 30.5% response rate. The final sample was 3,274 restaurants that responded to the paper survey. We arrived at 3000 as a sample size to ensure enough statistical power in our sample. 

We use stratified random sampling to obtain a sample that best represents the entire population. Unlike simple random sampling, which randomly selects data from the entire population, stratified random sampling takes each stratum in direct proportion to the population in each region compared to the total population in GTA. Stratified random sampling reflects the population more accurately than simple random sampling. With simple random sampling, it is not guaranteed that any particular subgroup or type of restaurant is chosen [@citeprocon]. In contrast, SRS ensures each subgroup within the population has proper representation within the sample. In other words, it captures key population characteristics in the sample. Stratification gives a smaller error in estimation and greater precision than the simple random sampling method. The greater the differences between the strata, the greater the gain in precision[@citeSRS1]. 

The collection instrument for this survey was paper questionnaires sent by mail. We first sent consent forms to obtain permission within the early stages of the experiment to randomly selected restaurants in a list of all restaurants per region. We did not use electronic questionnaires or surveys sent by email in order to prevent a non-response bias from the few restaurants that do not have a commercial email address or who don’t check their email frequently. Additionally, we avoided choosing in-person interviews in order to reduce close contact and lessen the chance of virus transmission. 

The total estimated cost was approximately \$20,958. To be specific, the average cost to print a page for black and white is around 5 cents [@citeprintcost]. According to Stamps, the cost of a one-ounce First Class Mail stamp is \$0.55 for one way [@citestampcost]. Each envelope cost 15 cents [@citeenevlopcost]. We sent 11,000 informed consents first, and then sent 3454 paper surveys after. When we mail the consent form and survey, we include a prepaid envelope with a stamp in each single package to encourage survey response. Therefore, we spent \$15,950 on consent forms and \$5,008 on paper surveys. See more details in [Appendix B](#appendixB).

We took three steps to deal with the non-responses. First of all, we tested the survey before sending them out. We kept the survey short. A one-minute survey normally has a higher response rate than a 15 minutes survey [@citenonresponse]. Besides, we sent reminders to the participants through mail or phone, if we did not receive responses in the first week. It ensured the surveys were sent to the right address. In addition, we would offer incentives in exchange for completing the survey. 

During the early stage of designing the survey, we determined that it was unnecessary to collect the personal information of the restaurant owner. Also, the survey did not ask the restaurant's name and address but only the first three digits of postal code (FSA code), which encodes region-level information. The survey was conducted anonymously; therefore, the risk of unauthorized collection, use, and disclosure of personal information was kept to a minimum. 

We did not use an electronic questionnaire sent by email. An email survey will require participants to send their responses back to us by personal email. Meanwhile, physical questionnaires can be sent back anonymously, without the sender's name and return address. 

In the questionnaire, most questions are multiple-choice, and we were not asking any open-end questions about the personal experience. In particular, when it comes to the cost structure of restaurant operations, which is deemed as sensitive questions by business owners, only the direction of change was asked instead of a specific amount. Thus, it would be impossible to reverse engineer the financial details and to infer private attributes about restaurants such as store names and locations using this dataset. 

When conducting the survey, we strictly follow the terms of reference for the safe collection, retention, use, disclosure, and disposal of personal information, in accordance with the Act. During the survey, we reviewed the term of reference periodically. At the final stage of the experiment, we checked all conditions and made sure they were fully complied with.
 
Participation in the survey was considered on a voluntary basis. Thus, we sent out the informed consent in order to provide participants as much information as possible. The consent should include the following:

* The main purpose of the research.
* The name of the institution that conducts the research.
* How the information will be disclosed.
* How much time the survey will take. 
* How the respondent will be informed about the final result. 

We have also considered how the data will be stored for future use. All data will be replaced with code and stored separately from the survey response. Petit Poll  will be responsible for ensuring the confidentiality, integrity, and accessibility of the dataset under supervision of the Ontario government. 



## Descriptive Analysis
```{r read_data, include=FALSE}

setwd("~/Downloads/Git/groupproject-") 

# read raw dataset from scripts, need to run survey_response _simulation first.
simulated_data<- read_csv("inputs/simulated_data.csv") 

# We add a new column To distinguish the restaurant size by employees number. 
# if employees number >20 then we define it is big, otherwise it is small.
for (i in 1:nrow(simulated_data)){
  if ((simulated_data$Q8[i] == ">30") || (simulated_data$Q8[i] == "20-30")){
  simulated_data$size[i] = "Big"  
  }else {
    simulated_data$size[i] = "Small"
  }
}

treated_data<-
  filter(simulated_data, simulated_data$type == "Treated" ) # load treated data 

control_data<-
  filter(simulated_data, simulated_data$type == "Control" ) # load control data
```

```{r size,echo=FALSE}
# Get the number of rows
size<- nrow(simulated_data)
```

After discussing data gathering method, we sampled data in R [@citeR]. We totally have **`r size`** observations, and 14 of following features according to the questionnaires.

* `type` : Categorical identifier [“Treated” or “Control”] for each observation 
* `Q1` : First three digits of the postcode 
* `Q2` : Categorical identifier for distinguishing the type of restaurants 
* `Q3` : Region name in GTA
* `Q4` : Describe whether the restaurant is a franchise (“Franchise” or “No”)
* `Q5` : The length of the operation years for each restaurant 
* `Q6` : Describe whether the restaurant offer takeout service (“Yes” or “No”)
* `Q7` : Describe whether the restaurant offer delivery service (“Yes” or “No”)
* `Q8` : Number of employees in the restaurant (category type)
* `Q9` : Average employee hourly rate (CAD) 
* `Q10` : Describe whether the restaurant has been a site of a potential COVID case (“Yes” or “No”)
* `Q11` : Describe the restaurant’s fixed costs change situation 
* `Q12` :  Describe the restaurant’s flexible costs change situation 
* `Q13` : The restaurant’s past month revenue (CAD)

The first six rows of raw data is shown in the Table1. (Table \@ref(tab:rowdata))
```{r rowdata, echo=FALSE}
#plot row data set in table
  head(simulated_data)%>%
  select(-size)%>%
  kableExtra::kbl(caption = "First 6 rows Raw data ") %>%
  kableExtra::kable_styling(latex_options = "scale_down")%>% # use scale_down option to make the font
  kable_styling()
```
### EDA

Taking a deep look at all the features from survey questionnaire, we learned some demographic features about the restaurants in GTA:

* From figure1(Figure \@ref(fig:restaurant1)) and figure2(Figure \@ref(fig:restaurant2)), we noticed that more restaurants are located in Toronto (around 500) and Peel (around 400). The number of restaurants in Hilton is similar to the number in Durham. Meanwhile, Casual dining takes the lead in the restaurant type in GTA, with around 26%. Then it comes to Family style restaurant, accounting for 20%. Fast food restaurant, fine dining restaurant and Premium casual restaurant almost equally make up 10%.
There is no big difference between treated group and control group in terms of restaurant number and type distributions.


```{r restaurant1, fig.cap = "Restaurant numbers and types", echo=FALSE,fig.width=8, fig.height=5}
R1<-simulated_data%>%
  ggplot(aes(x = simulated_data$Q2,fill = type)) +
  geom_bar(stat = "count", position = "dodge") +
  labs(title = "More restuarants are loacted in City of Toronto and Peel Region",x="Regions") +
  theme_minimal() +
  scale_fill_manual(values = c("#E85C34", "#2C92D5"))

R2<-simulated_data%>%
  ggplot(aes(x = simulated_data$Q3 ,fill = type)) +
  geom_bar(stat = "count", position = "dodge") +
  labs(title = "More restuarants in GTA are casual dinning ",x="Regions") +
  theme_minimal() +
  scale_fill_manual(values = c("#E85C34", "#2C92D5"))

# combine two diagram in one graph
ggarrange(
  R1,R2,
  ncol = 1, nrow = 2,
  vjust = 1,
  widths = c(2, 2),
  align = "hv",
  common.legend = TRUE, legend = "bottom"
  )

```


```{r restaurant2, fig.cap = "Restaurant types", echo=FALSE,fig.width=12, fig.height=4}

library(viridisLite)

# Treated group
rt1<-
  ggplot(treated_data, aes(fill= Q3, x= Q2)) +
  geom_bar(position="stack", stat="count")+
    ggtitle("Treated Group: Restaurant Types") +
    xlab("Regions")+
    ylab("Count of Numbers")+
    theme_grey()

    
rt1<- rt1 + guides(fill=guide_legend(title="Types")) # change the Legend title 

# Control group
rt2<-
  ggplot(control_data, aes(fill= Q3, x= Q2)) +
  geom_bar(position="stack", stat="count")+
    ggtitle("Control Group: Restaurant Types") +
    xlab("Regions")+
    ylab("Count of Numbers")+
    theme_grey()
    
rt2<- rt2 + guides(fill=guide_legend(title="Types")) # change the Legend title 
# combine two diagram in one graph
ggarrange(
  rt1, rt2, 
  ncol = 2, nrow = 1,
  vjust = 1,
  widths = c(2, 2),
  align = "hv",
  common.legend = TRUE, legend = "bottom"
  )

```

* In terms of employees salary, the average hourly rate before and after intervention is both around 17 CAD. In contrast with two boxplots, we can see there is a slight increase in the treated group. The reason behind might because the employee take more risks to go for work, accordingly they will receive higher salary.  (Figure \@ref(fig:salary))  

```{r salary distribution, echo=FALSE}
sd1<-
  ggplot(simulated_data, aes(x = simulated_data$Q9, fill=type)) +
  geom_histogram(position = "dodge", bins = 30,binwidth = 0.5) +
  scale_fill_manual(values = c("#E85C34", "#2C92D5"))+
  labs(title="The salary distributions was higher for teatment group", x="Salary", y="Number of employee")+
  geom_density()
```


```{r salary, fig.cap = "Employee salary distribution",echo=FALSE, fig.width=10, fig.height=4}
library(ggthemes)
# Treated group
s1<-
  ggplot(treated_data, aes(x = Q2, y= Q9)) +
  geom_boxplot(outlier.colour="red", 
               outlier.shape=5,
               outlier.size=4) +
  labs(title="Salary Distributions in Treated Group", x="Regions", y="Hourly rate")+
  geom_dotplot(binaxis='y', stackdir='center', dotsize=0.01, binwidth =1)+ # add dots
  theme_economist()

# Control group
s2<-
  ggplot(control_data, aes(x = Q2, y= Q9)) +
  geom_boxplot(outlier.colour="red", 
               outlier.shape=5,
               outlier.size=4)+
  labs(title="Salary Distributions in Control Group", x="Regions", y="Hourly rate")+
  geom_dotplot(binaxis='y', stackdir='center', dotsize=0.01,binwidth =1)+ #add dots
  theme_economist()


# combine two diagram in one graph

library(ggpubr)
ggarrange(
  s1, s2, 
  ncol = 2, nrow = 1,
  hjust = -0.5,
  align = "hv"
  )

```


* The attributes of the restaurant determine its management mode, so whether the restaurant is a franchise is quite important factor.[@nhamo2020restaurants] From the pie charts, we found the portion of franchise rate in treated group is slightly lower than control group. We assume in treated group, the branch franchise restaurants should follow the rules by the head office. Considering the potential cost of COVID issues, chain restaurants will face greater risks, which is why they are less likely to be in the treated group. 
(Figure \@ref(fig:pie1))


```{r pie1, fig.cap = "Restaurant franchise distribution", echo=FALSE, fig.width=10, fig.height=5}

Pie1<-control_data%>%
  ggplot(aes(x = control_data$Q4 ,fill = Q4)) +
  geom_bar(stat = "count", position = "dodge") +
  coord_polar("y", start=0)+   #Create a pie chart based on the barplot
  labs(title = "Franchise restaurants slightly decrease in treated group ",x="Regions",y="Count in control group") +
  theme_minimal() +
  scale_fill_manual(values = c("#E85C34", "#2C92D5"))

Pie2<-treated_data%>%
  ggplot(aes(x = treated_data$Q4 ,fill = Q4)) +
  geom_bar(stat = "count", position = "dodge") +
  coord_polar("y", start=0)+
  labs(title = "",x="Regions", y="Count in treated group") +
  theme_minimal() +
  scale_fill_manual(values = c("#E85C34", "#2C92D5"))

#Merge twp graphs
ggarrange(
  Pie1, Pie2, 
  ncol = 2, nrow = 1,
  hjust = -0.5,
  align = "hv"
  )
```

* The polar chart illustrates the employee numbers distribution, as can be seen in figure below. (Figure \@ref(fig:employee)) Because of COVID rule, no restaurant is allowed to open for large group dine in. For large size restaurant, they all make corresponding actions to reduce their flexiable costs, such as reduce the full time employees. So in the control group, there is 0 restaurant which has more than 30 employees. Most of restaurant has 10 to 20 employees. 

```{r employee, fig.cap = "Employee numbers distribution",echo=FALSE, fig.width=10, fig.height=4}
cxc <- 
  simulated_data%>%
  ggplot(aes(x = simulated_data$Q8 ,fill = type)) +
  geom_bar(width = 0.7,position = "dodge") + 
  coord_polar()+
  scale_fill_manual(values = c("#E85C34", "#2C92D5"))+
  labs(title = "Almost 0 restaurant has more than 30 employees in control group",x="", y="Count") +
  theme_minimal()

cxc
```




### T-Test
The T Test is used to compare the sample mean of our Treated group and Control group. The goal is to determine whether the intervention has an effective effect on the treated group. Our hypothesis is the intervention will have positive impact towards the restaurant's revenue. [@kim2015t] The T test results is represented in the Table2(Table \@ref(tab:ttest)). The package **Broom**[@broom1] is used to clean the t test results and convert it into the dataframe. The p value we get is < 2.2e-16, as the p value would indicate a significant result, meaning that the actual p value is even smaller than 2.2e-16 (a typical threshold is 0.05, anything smaller counts as statistically significant).[@kim2015t] So we can 
interpret hypothesis not rejected which means the intervention has a significant effect on treated group.

```{r ttest, echo=FALSE}
#install.packages("broom")
library(broom)
revenue_c <- control_data$Q13
revenue_t <- treated_data$Q13
t<-t.test(revenue_t, revenue_c)
m<-tidy(t, digits=1)
m%>%
  select(-c(estimate,statistic,parameter))%>%
  rename(
    mean_of_Treated=estimate1,
    mean_of_Control=estimate2
  )%>%
  kableExtra::kbl(caption = "T Test on the Restaurant's revenue") %>%
  kableExtra::kable_styling(latex_options = "scale_down")%>% # use scale_down option to make the font
  kable_styling()
  
```

### Correlation matrix
Correlation matrix shows internal relationships between x variables and y variable. (Figure \@ref(fig:correlation)) Intensity is indicated by the color(from red to blue). No significant coeffiency   is barred with symbol "x". More detailed analysis will be conducted in finding part. 

```{r correlation, fig.cap = "Correlation matrix",echo=FALSE, fig.width=10, fig.height=10}
#install.packages("fastDummies")
library(fastDummies)
Dummy_data<-simulated_data[-c(1,2)] %>% #Remove Type and postcode columns
rename(
  Region = Q2,
  operation_years = Q5,
  Hourly_rate = Q9,
  Total_renvenue =Q13,
  Type=Q3,
  Fanchise= Q4,
  Takeout=Q6,
  Delivery=Q7,
  Size=Q8,
  COVID = Q10,
  Fixed=Q11,
  Flex=Q12
)
  
  
  
Dummy_data<- fastDummies::dummy_cols(Dummy_data) #  change categorical var to factors 

# correlation matrix for restaurants in treated group
Dummy_data<-Dummy_data[-c(1:3,5:7,9:11,13,44)] # remove big restaurant, cause it will always has -1 relationship with small restaurant

Dummy_data<-   # change column names
  Dummy_data

# Compute a correlation matrix
corr <- round(cor(Dummy_data), 1) # round to 1 digit

# Compute a matrix of correlation p-values
#install.packages("ggcorrplot")
library(ggcorrplot)
p.mat <- cor_pmat(Dummy_data)

# Visualize the lower triangle of the correlation matrix
# Barring the no significant coefficient
ggcorrplot(
  corr, hc.order = TRUE, type = "lower",
  p.mat=p.mat,
  title = "Correlation Matrix"
  )

```








# Discussion

## Overview 

## Findings 

### Finding ONE

### Finding TWO
Invention effect on Flex and fiexed cost 

```{r felx_fix, fig.cap = "Invention effect on Flex and fiexed cost",echo=FALSE, fig.width=6, fig.height=4}
# Treated group

# Fixed costs 
fixc1<-
  ggplot(simulated_data, aes(fill= type, x=Q11)) +
  geom_bar(stat="count", width = 0.6,position = "dodge")+
  scale_fill_manual(values = c("#E85C34", "#2C92D5"))+
    ggtitle("Fixed costs increased in treated group and remain same in control group") +
    coord_flip()+
    xlab("Changes")+
    ylab("Number of restaurants")+
    theme_minimal()
fixc1<- fixc1 + guides(fill=guide_legend(title="Changes")) # change the Legend title 

# flex cost
flexc1<-
    ggplot(simulated_data, aes(fill= type, x=Q12)) +
    geom_bar(stat="count", width = 0.7, position = "dodge")+
    scale_fill_manual(values = c("#E85C34", "#2C92D5"))+
    ggtitle("Flexible costs increased in most of treated group restaurants") +
    coord_flip()+
    xlab("Changes")+
    ylab("Number of restaurants")+
    theme_minimal()
flexc1<- flexc1 + guides(fill=guide_legend(title="Changes")) # change the Legend title 



# combine two diagram in one graph
ggarrange(
  flexc1,fixc1,
  ncol = 1, nrow = 2,
  vjust = 1,
  widths = c(2, 2),
  align = "hv",
  common.legend = TRUE, legend = "bottom"
  )

```
### Finding THREE
Invention effect on Revenue distributions 

```{r revenue_distribution, fig.cap="Invention effect on Revenue distributions",echo=FALSE}

library(ggthemes)
# Treated group
rd1<-
  ggplot(simulated_data, aes(x = simulated_data$Q13/1000, fill=type)) +
  geom_histogram(position = "dodge", bins = 30,binwidth = 5) +
  scale_fill_manual(values = c("#E85C34", "#2C92D5"))+
  labs(title="Revenue Distributions Move to the right", x="Renvenue (Thousands)", y="Count")+
  geom_density()+
  theme_calc()

rd1

```
## Limitation

## Future Directions 

```{r}

```

\newpage
# Appendix

## Appendix A {#appendixA}
```{r AppendixA, message=FALSE, echo = FALSE}
### Number in GTA ### 
number_of_Toronto = 7500
number_of_Durham = 3260
number_of_York = 5553
number_of_Peel = 6235
number_of_Halton = 2803
number_of_GTA = sum(number_of_Toronto,number_of_Durham,number_of_York,number_of_Peel,number_of_Halton)

### Proportion ###
proportion_Toronto = (number_of_Toronto/number_of_GTA*100) 
proportion_Durham = (number_of_Durham/number_of_GTA*100)
proportion_York = (number_of_York/number_of_GTA*100)
proportion_Peel = (number_of_Peel/number_of_GTA*100)
proportion_Halton = (number_of_Halton/number_of_GTA*100)
Proportion_GTA = (number_of_GTA /number_of_GTA*100)

### Number of Sample ###
number_of_sample = 1637

sample_Toronto =proportion_Toronto*number_of_sample
sample_Durham =proportion_Durham *number_of_sample
sample_York =proportion_York*number_of_sample
sample_Peel =proportion_Peel*number_of_sample
sample_Halton =proportion_Halton *number_of_sample

### Dataframe ###
sample_size <- data.frame(
  Region = c ("Toronto", "Durham" , "York" , "Peel" , "Halton","Total"),
  Number_of_Region = c (number_of_Toronto,number_of_Durham,number_of_York,number_of_Peel,number_of_Halton,number_of_GTA),
  Proportion_of_Region= c (proportion_Toronto,proportion_Durham,proportion_York,proportion_Peel,proportion_Halton,Proportion_GTA)%>% round(digits = 2),
  Sample_of_Region = c (sample_Toronto,sample_Durham,sample_York ,sample_Peel,sample_Halton,number_of_sample) %>% round(digits = 0)
  )
  
### Table ###
library(kableExtra)
sample_size %>% 
  knitr::kable(caption = "Detailed information for stratification",
               col.names = c("Region", "Number of Restuarants", "Proportion(%)", "Sample Selected"))%>%
  kable_styling()
```

## Appendix B {#appendixB}
```{r AppendixB, message=FALSE, echo = FALSE}
### basic information ###
number_of_survey = 3454
number_of_consent = 11325

print_cost_per_page = 0.05 
envelope_cost = 0.15
one_way_stamp = 0.55

### Calculation ###

total_print_cost = print_cost_per_page* (number_of_consent+number_of_survey)
total_envelope_cost = (number_of_consent+number_of_survey)*envelope_cost*2
total_stamp_cost =(number_of_consent+number_of_survey)*one_way_stamp*2

total_cost = total_print_cost+total_envelope_cost+total_stamp_cost

### dataframe ###
estimated_cost <- data.frame(
  Components = c ("Printing Cost", "Envelope Cost", "Stamp Cost"),
  Cost_per_unit= c (print_cost_per_page,envelope_cost,one_way_stamp),
  Total_cost_of_each_component  = c (total_print_cost, total_envelope_cost, total_stamp_cost)
)

### Table  ###
library(kableExtra)
estimated_cost %>% 
  knitr::kable(caption = "Estimated Cost",
               col.names = c("Components", "Cost per unit", "Total cost for each component"))%>%
  kable_styling()


```
## Appendix C: Screenshot of the survey



\newpage

# References
